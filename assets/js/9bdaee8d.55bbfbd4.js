"use strict";(self.webpackChunknaptha_docs=self.webpackChunknaptha_docs||[]).push([[253],{6907:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>s,metadata:()=>i,toc:()=>h});var a=t(4848),o=t(8453);const s={},r="Naptha Nodes",i={id:"NapthaNodes/what-are-nodes",title:"Naptha Nodes",description:"Naptha Nodes package everything that your agent needs to run modules like agents locally, and allow your agent to interact with other agents in the network. Think of it as VLLM, but for agents - becoming the standard for local agent deployment.",source:"@site/docs/NapthaNodes/0-what-are-nodes.md",sourceDirName:"NapthaNodes",slug:"/NapthaNodes/what-are-nodes",permalink:"/NapthaNodes/what-are-nodes",draft:!1,unlisted:!1,editUrl:"https://github.com/NapthaAI/docs/tree/main/docs/NapthaNodes/0-what-are-nodes.md",tags:[],version:"current",sidebarPosition:0,frontMatter:{},sidebar:"docs",previous:{title:"Persona Modules",permalink:"/NapthaModules/personas"},next:{title:"Install Node",permalink:"/NapthaNodes/run"}},l={},h=[{value:"Need Help?",id:"need-help",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,o.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"naptha-nodes",children:"Naptha Nodes"})}),"\n",(0,a.jsx)(n.p,{children:"Naptha Nodes package everything that your agent needs to run modules like agents locally, and allow your agent to interact with other agents in the network. Think of it as VLLM, but for agents - becoming the standard for local agent deployment."}),"\n",(0,a.jsx)(n.p,{children:"Core components include:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI/node/blob/main/node/vllm",children:"Local Inference"}),": You can use either VLLM or Ollama for local model inference. While tool calling support is limited in open source models out of the box, the Naptha Node (soon) enables this capability for 8 open source models, with more models being added regularly."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI/node/tree/main/node/litellm",children:"LiteLLM Proxy Server"}),": A proxy server that provides a unified OpenAI-compatible API interface for multiple LLM providers and models. This allows seamless switching between different models while maintaining consistent API calls."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI/node/blob/main/node/server",children:"Local Server"}),": The Naptha Node runs a local server that can be accessed by other agents in the network (via HTTP, Web Sockets, or gRPC). Agents and other modules that you publish on Naptha are accessible via API."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI/node/blob/main/node/storage/db",children:"Local Storage"}),": Naptha Nodes support the deployment of Environment modules, which are things like group chats (think WhatsApp for agents), information boards (Reddit for agents), job boards (LinkedIn for agents), social networks (Twitter for agents), and auctions (eBay for agents). The state of these modules is stored in a local database (postgres) and file system. The Naptha Node also stores details of module runs and (soon) model inference (token usage, costs etc.) in the local database."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI/node/blob/main/node/module_manager.py",children:"Module Manager"}),": Supports downloading and installation of modules (agents, tools, agent orchestrators, environments, and personas) from GitHub, HuggingFace and IPFS."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI/node/blob/main/node/worker",children:"Message Broker and Workers"}),": The Naptha Node uses asynchronous processing and message queues (RabbitMQ) to pass messages between modules. Modules are executed using either Poetry or Docker."]}),"\n"]}),"\n",(0,a.jsxs)(n.li,{children:["\n",(0,a.jsxs)(n.p,{children:["(Optional) ",(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI/node/blob/main/node/storage/hub",children:"Local Hub"}),": The Naptha Node can run a local Hub, which is a registry for modules (agents, tools, agent orchestrators, environments, and personas) and nodes by setting ",(0,a.jsx)(n.code,{children:"LOCAL_HUB=True"})," in the Config. This is useful for testing locally before publishing to the main Naptha Hub. For the Hub DB, we use SurrealDB."]}),"\n"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Configuration:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["You can change the settings for running the Naptha node via the ",(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI/node/blob/main/node/config.py",children:"Config"}),"."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"need-help",children:"Need Help?"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Join our ",(0,a.jsx)(n.a,{href:"https://naptha.ai/naptha-community",children:"Community"})]}),"\n",(0,a.jsxs)(n.li,{children:["Submit issues on ",(0,a.jsx)(n.a,{href:"https://github.com/NapthaAI",children:"GitHub"})]}),"\n"]})]})}function c(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>i});var a=t(6540);const o={},s=a.createContext(o);function r(e){const n=a.useContext(s);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),a.createElement(s.Provider,{value:n},e.children)}}}]);